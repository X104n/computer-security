{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine learning grop exam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Group members:\n",
    "* Stian Munkejord\n",
    "* Oskar Krystian Michalski\n",
    "* Ã˜sten Vestby Edvartsen\n",
    "* Robin Gamlem Hjelvik"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the first chapter of the book they specify:\n",
    "\n",
    "'Throughout the book we make ample use of NumPy, matplotlib and pandas. All the code will assume the following imports:'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#TO Encode, Scale and split data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#The Models we are going to use\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#To make a print_score function\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Load the data and preprocess it\n",
    "dataset =  pd.read_csv(\"agaricus-lepiota.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#dataset.head()\n",
    "#dataset.info() #Shows that every row has non-null values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Wanted to see a heatmap of the correletaiom between the different values and collum.\n",
    "#This will show the  most indicative features in the dataset and the most domentating feutures.\n",
    "#corr = dataset.corr()\n",
    "#sns.heatmap(corr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#Need to drop some coloms because of the logical rules in the dataset.\n",
    "#This is features that are the most indicative, and we would therefore drop these before running the models.\n",
    "dataset = dataset.drop(columns='p.1') #Odor\n",
    "dataset = dataset.drop(columns='k.1') #Spore-print-color\n",
    "dataset = dataset.drop(columns='u') #Habitant\n",
    "dataset = dataset.drop(columns='s.3') #Population\n",
    "dataset = dataset.drop(columns='s.1')#Above\n",
    "dataset = dataset.drop(columns='s.2')#below\n",
    "dataset = dataset.drop(columns='p.3')#ring-type\n",
    "dataset = dataset.drop(columns='e.1')#stalk-root\n",
    "dataset = dataset.drop(columns='k')#gill-color\n",
    "dataset = dataset.drop(columns='t')#bruises?\n",
    "dataset = dataset.drop(columns='c')#gill-spacing\n",
    "dataset = dataset.drop(columns='n.1')#gill-size\n",
    "dataset = dataset.drop(columns='e')#stalk-shape\n",
    "dataset = dataset.drop(columns='n')#cap-color"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#Label Encoding, so that the cells contain corresponding number for the character, and replaces it.\n",
    "Encoder = LabelEncoder()\n",
    "for col in dataset.columns:\n",
    "    dataset[col] = Encoder.fit_transform(dataset[col])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 'p'/class is the column that decides if the mushroom is edible or poisounus.\n",
    "X=dataset.drop('p',axis=1) #Predictors 'p'=\"class\", This is what we are trying to predict.\n",
    "y=dataset['p'] #Response, the data we have to work with, is then the rest of the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X = pd.DataFrame(scalar.fit_transform(X), columns = X.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=38)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#We want to use KNN, DecisionTreeclassifier and RandomForrestClassifier because"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(oob_score=True)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "knn = KNN()\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=5, criterion=\"entropy\")\n",
    "dtc.fit(X_train,y_train)\n",
    "\n",
    "#RandomForrestClassifier\n",
    "rfc = RandomForestClassifier(oob_score=True)\n",
    "rfc.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#A function to print a classification report for the different models.\n",
    "def print_score(classifier,X_train,y_train,X_test,y_test, name):\n",
    "    print(\"Training results for\", name, \":\\n\")\n",
    "    print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results for KNN :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83      2945\n",
      "           1       0.87      0.72      0.79      2741\n",
      "\n",
      "    accuracy                           0.81      5686\n",
      "   macro avg       0.82      0.81      0.81      5686\n",
      "weighted avg       0.82      0.81      0.81      5686\n",
      "\n",
      "\n",
      "Training results for DTC :\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.52      0.67      2945\n",
      "           1       0.65      0.96      0.78      2741\n",
      "\n",
      "    accuracy                           0.73      5686\n",
      "   macro avg       0.79      0.74      0.72      5686\n",
      "weighted avg       0.80      0.73      0.72      5686\n",
      "\n",
      "\n",
      "Training results for RFC :\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      2945\n",
      "           1       0.80      0.88      0.84      2741\n",
      "\n",
      "    accuracy                           0.83      5686\n",
      "   macro avg       0.84      0.84      0.83      5686\n",
      "weighted avg       0.84      0.83      0.83      5686\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(knn,X_train,y_train,X_test,y_test,\"KNN\")\n",
    "print_score(dtc,X_train,y_train,X_test,y_test,\"DTC\")\n",
    "print_score(rfc,X_train,y_train,X_test,y_test,\"RFC\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Our best performing model is the RandomForrest classifier, as the average score is 83%. We would still not trust the model as there is still 20% that is wrong, and that can be dangerous. The dataset was originally sufficient because it would give a score of 100% for all three models. To really test the models we would need to decrease the numbers of colums and avoid the logical rules of the dataset.\n",
    "\n",
    "The decision tree classifier is a tree-like model that splits the data based on the most significant feature until a prediction can be made.\n",
    "We think the random forrest classifier perform best because it is an ensemble of decision trees, where each tree makes a prediction and the final prediction is made by combining the predictions of all trees. Therefore, the RFC is even more precise than the dtc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This first part of the task I will be evaluating my preprocessing. Before starting with the real Sentiment Analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# Extracts only the text part of the document.\n",
    "def data_processing(file_name) -> list:\n",
    "    message_data = []\n",
    "    with open(f'{file_name}', \"r\") as training_data:\n",
    "        train_array = json.load(training_data)\n",
    "\n",
    "        #Train_array is a array with dictionaries, each having three elements each.\n",
    "        for category in train_array:\n",
    "            #category is the index of the list with the dictionary.\n",
    "\n",
    "            message_data.append(category[\"text\"])\n",
    "\n",
    "    return message_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I ask my group leader who gave a good advice when it comes to tokenizing data and removing the stopwords.\n",
    "\n",
    "He adviced me to sanitize the data by removing the tokens while tokenizing them, which was a smart way of avoiding unnecessery high amount of list accesses. The function sentance_sanitized_tokening() takes the list with full sentances, and then iterates through each tokenized word in the sentance, in order to check if it exists in dummyWords which is a english stopword list from nltk, if the word exists in the list it just goes to the next iteration/word. If it ain't then it adds it to the list of words that is to be added to the output list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentance tokenizing and stopword removing function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentance_sanitized_tokening(data: dict) -> list:\n",
    "    tokenized_list = []\n",
    "    dummyWords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # data_part = [\"text\", \"label\"]\n",
    "    for data_part in data:\n",
    "        sentance, label = data_part\n",
    "        tokened_sentance = nltk.word_tokenize(sentance)\n",
    "\n",
    "        # Each sentance has it's own filtered_tokens list containing the words that is not stopwords.\n",
    "        filtered_tokens = []\n",
    "\n",
    "        for word in tokened_sentance:\n",
    "            assert type(word)==str, \"Words inside the dictionary passed to tokenize(data) is not of type string!!\"\n",
    "            # I considered using remove() function but that would have a very high impact on programs runtime which I prefere to avoid.\n",
    "\n",
    "            if word not in dummyWords:\n",
    "                filtered_tokens.append(word)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # When all the words from the sentance that were not stopwords was added then append them to the output (tokenized_list)\n",
    "        tokenized_list.append([filtered_tokens, label])\n",
    "\n",
    "    return tokenized_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing I considered was weather the symbol \"!\" should be removed. It may be used to help characterized Negative words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But as"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23924843510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from collections import Counter\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = '/cifar-10-batches-py'\n",
    "\n",
    "tensor_cifar10_train_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train dataset:         45000\n",
      "Size of the validation dataset:    5000\n",
      "Size of the test dataset:          10000\n"
     ]
    }
   ],
   "source": [
    "n_val = 5000\n",
    "n_train = len(tensor_cifar10_train_val)-n_val\n",
    "\n",
    "\n",
    "cifar10_train, cifar10_val = random_split(\n",
    "    tensor_cifar10_train_val,\n",
    "    [n_train, n_val],\n",
    "    generator=torch.Generator().manual_seed(123)\n",
    ")\n",
    "\n",
    "print(\"Size of the train dataset:        \", len(cifar10_train))\n",
    "print(\"Size of the validation dataset:   \", len(cifar10_val))\n",
    "print(\"Size of the test dataset:         \", len(cifar10_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 4504,\n",
       "         4: 4510,\n",
       "         2: 4504,\n",
       "         1: 4471,\n",
       "         9: 4520,\n",
       "         8: 4506,\n",
       "         0: 4513,\n",
       "         6: 4517,\n",
       "         7: 4505,\n",
       "         5: 4450})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([label for _, label in cifar10_train])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.stack([img for img, _ in tensor_cifar10_train_val])\n",
    "print(imgs.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizing the dataset around the mean and standard deviation of the 3 dimensions (height, width channel (color))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalizer = transforms.Normalize(mean = imgs.mean(dim=(0, 2, 3)), std = imgs.std(dim=(0, 2, 3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the dataset and applying the composition of transforms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensor_cifar10_train_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform = transforms.compose([transforms.ToTensor, normalizer])\n",
    ")\n",
    "\n",
    "transformed_cifar10_test = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.compose([transforms.ToTensor, normalizer])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attempt at plotting the normalized images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_t, _ = tensor_cifar10_train_val[1]\n",
    "\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
