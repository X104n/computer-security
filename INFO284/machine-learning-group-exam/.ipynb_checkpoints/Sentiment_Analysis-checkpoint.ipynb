{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part of the task I will be evaluating my preprocessing. Before starting with the real Sentiment Analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# Extracts only the text part of the document. \n",
    "def data_processing(file_name) -> list:\n",
    "    message_data = []\n",
    "    with open(f'{file_name}', \"r\") as training_data:\n",
    "        train_array = json.load(training_data)\n",
    "\n",
    "        #Train_array is a array with dictionaries, each having three elements each. \n",
    "        for category in train_array:\n",
    "            #category is the index of the list with the dictionary.\n",
    "            \n",
    "            message_data.append(category[\"text\"]) \n",
    "\n",
    "    return message_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ask my group leader who gave a good advice when it comes to tokenizing data and removing the stopwords. \n",
    "\n",
    "He adviced me to sanitize the data by removing the tokens while tokenizing them, which was a smart way of avoiding unnecessery high amount of list accesses. The function sentance_sanitized_tokening() takes the list with full sentances, and then iterates through each tokenized word in the sentance, in order to check if it exists in dummyWords which is a english stopword list from nltk, if the word exists in the list it just goes to the next iteration/word. If it ain't then it adds it to the list of words that is to be added to the output list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentance tokenizing and stopword removing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentance_sanitized_tokening(data: dict) -> list:\n",
    "    tokenized_list = []\n",
    "    dummyWords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # data_part = [\"text\", \"label\"]\n",
    "    for data_part in data:\n",
    "        sentance, label = data_part\n",
    "        tokened_sentance = nltk.word_tokenize(sentance)\n",
    "        \n",
    "        # Each sentance has it's own filtered_tokens list containing the words that is not stopwords. \n",
    "        filtered_tokens = []\n",
    "        \n",
    "        for word in tokened_sentance:\n",
    "            assert type(word)==str, \"Words inside the dictionary passed to tokenize(data) is not of type string!!\"\n",
    "            # I considered using remove() function but that would have a very high impact on programs runtime which I prefere to avoid. \n",
    "     \n",
    "            if word not in dummyWords:\n",
    "                filtered_tokens.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # When all the words from the sentance that were not stopwords was added then append them to the output (tokenized_list)\n",
    "        tokenized_list.append([filtered_tokens, label])\n",
    "        \n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I considered was weather the symbol \"!\" should be removed. It may be used to help characterized Negative words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5a8d3d6fa4a8c42668baa170a0b6198815339ab86606f8fa92c48503601c5fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
