{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc4x0R-UpLKx"
      },
      "source": [
        "In this task I will have a couple of tasks to performe in my process of sentiment analysis:\n",
        "## Sentiment Analysis process:\n",
        "    1.  Analyze the Data and understand how I should handle it.\n",
        "    2.  Load the necessary data and filter out redundant information.\n",
        "    3.  Handle the data and prepare it such that it fits the models I will be using. This step will contain a couple of subprocess:\n",
        "        a. Tokenizing the data.\n",
        "        b. Sanitizing the data by removing all the stopwords, punctuation and numbers that will only reduce the performance of the Sentiment Analysis model. \n",
        "        c. Lemmatizing the data in order to make it more understandable for the model.\n",
        "        d. Vectorizing in order to fit the data to an graph that I will be using when performing the real Sentiment Analysis. \n",
        "    4. Finding the most optimal algorithm.\n",
        "        a. I will have to try different algorithms with cross validation and use their results in order to find which one fits the data the best. \n",
        "        b. In addition I will need to find the most optimal parameters. I will see later how I choose to search for them. \n",
        "    5. Training the model with the preprocessed data.\n",
        "    6. Testing the model with the preprocessed data. \n",
        "\n",
        "And then I will be evaluating the data, to see if my score is sufficient or not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKs5KxeSpLK3",
        "outputId": "fa7a8385-c634-431b-c74d-d3f1069fb584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.1.0/nb_core_news_sm-3.1.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.1.0/nb_core_news_sm-3.1.0.tar.gz (16.1 MB)\n",
            "                                              0.0/16.1 MB ? eta -:--:--\n",
            "                                              0.0/16.1 MB 1.4 MB/s eta 0:00:12\n",
            "                                              0.0/16.1 MB 1.4 MB/s eta 0:00:12\n",
            "                                             0.1/16.1 MB 653.6 kB/s eta 0:00:25\n",
            "                                             0.1/16.1 MB 653.6 kB/s eta 0:00:25\n",
            "                                              0.2/16.1 MB 1.1 MB/s eta 0:00:15\n",
            "                                              0.3/16.1 MB 1.3 MB/s eta 0:00:13\n",
            "     -                                        0.5/16.1 MB 1.5 MB/s eta 0:00:11\n",
            "     -                                        0.7/16.1 MB 1.9 MB/s eta 0:00:09\n",
            "     --                                       0.8/16.1 MB 2.0 MB/s eta 0:00:08\n",
            "     --                                       1.0/16.1 MB 2.2 MB/s eta 0:00:07\n",
            "     --                                       1.2/16.1 MB 2.4 MB/s eta 0:00:07\n",
            "     ---                                      1.3/16.1 MB 2.5 MB/s eta 0:00:06\n",
            "     ---                                      1.3/16.1 MB 2.5 MB/s eta 0:00:06\n",
            "     ---                                      1.6/16.1 MB 2.5 MB/s eta 0:00:06\n",
            "     ----                                     1.9/16.1 MB 2.7 MB/s eta 0:00:06\n",
            "     -----                                    2.2/16.1 MB 3.0 MB/s eta 0:00:05\n",
            "     -----                                    2.3/16.1 MB 3.1 MB/s eta 0:00:05\n",
            "     ------                                   2.5/16.1 MB 3.1 MB/s eta 0:00:05\n",
            "     -------                                  3.0/16.1 MB 3.4 MB/s eta 0:00:04\n",
            "     --------                                 3.4/16.1 MB 3.7 MB/s eta 0:00:04\n",
            "     ---------                                3.8/16.1 MB 4.0 MB/s eta 0:00:04\n",
            "     ----------                               4.2/16.1 MB 4.2 MB/s eta 0:00:03\n",
            "     -----------                              4.7/16.1 MB 4.5 MB/s eta 0:00:03\n",
            "     ------------                             5.1/16.1 MB 4.6 MB/s eta 0:00:03\n",
            "     --------------                           5.7/16.1 MB 4.9 MB/s eta 0:00:03\n",
            "     ---------------                          6.1/16.1 MB 5.1 MB/s eta 0:00:02\n",
            "     ----------------                         6.8/16.1 MB 5.4 MB/s eta 0:00:02\n",
            "     -----------------                        7.2/16.1 MB 5.6 MB/s eta 0:00:02\n",
            "     -------------------                      7.8/16.1 MB 5.9 MB/s eta 0:00:02\n",
            "     --------------------                     8.3/16.1 MB 6.0 MB/s eta 0:00:02\n",
            "     ---------------------                    8.8/16.1 MB 6.2 MB/s eta 0:00:02\n",
            "     -----------------------                  9.4/16.1 MB 6.4 MB/s eta 0:00:02\n",
            "     ------------------------                 10.0/16.1 MB 6.6 MB/s eta 0:00:01\n",
            "     -------------------------                10.4/16.1 MB 7.5 MB/s eta 0:00:01\n",
            "     ---------------------------              10.9/16.1 MB 8.1 MB/s eta 0:00:01\n",
            "     ----------------------------             11.4/16.1 MB 8.8 MB/s eta 0:00:01\n",
            "     -----------------------------            11.9/16.1 MB 9.8 MB/s eta 0:00:01\n",
            "     ------------------------------           12.3/16.1 MB 9.8 MB/s eta 0:00:01\n",
            "     ------------------------------          12.6/16.1 MB 10.2 MB/s eta 0:00:01\n",
            "     -------------------------------         12.9/16.1 MB 10.1 MB/s eta 0:00:01\n",
            "     --------------------------------        13.4/16.1 MB 10.2 MB/s eta 0:00:01\n",
            "     ---------------------------------       13.9/16.1 MB 10.2 MB/s eta 0:00:01\n",
            "     ----------------------------------      14.4/16.1 MB 10.4 MB/s eta 0:00:01\n",
            "     ------------------------------------    14.9/16.1 MB 10.6 MB/s eta 0:00:01\n",
            "     -------------------------------------   15.6/16.1 MB 10.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  16.1/16.1 MB 10.9 MB/s eta 0:00:01\n",
            "     --------------------------------------  16.1/16.1 MB 10.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 16.1/16.1 MB 9.9 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nb-core-news-sm==3.1.0) (3.1.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.28.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# This is for the sentimental analysis more exactly for comparing the different algorithms with eachother. \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.1.0/nb_core_news_sm-3.1.0.tar.gz"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.Analyzing the data & understanding its structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MJXjXd5OpLK5"
      },
      "outputs": [],
      "source": [
        "def data_analysys(json_data) -> list:\n",
        "    keys_in_data = set()\n",
        "    print(\"The size of the data is:\", len(json_data))\n",
        "\n",
        "    for item in json_data:\n",
        "        keys = item.keys()\n",
        "        keys_in_data.update(keys)\n",
        "\n",
        "    print(f'Main keys of this data are: {keys_in_data}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will use the function above to analyze the structure of the content of this file. To learn how I should be preprocessing it. And how I can make it more efficient."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell below I simply load the documents from the url's and print the recieved data using the previously defined data_analysys() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the data is: 3894\n",
            "Main keys of this data are: {'sent_id', 'label', 'text'}\n",
            "The size of the data is: 583\n",
            "Main keys of this data are: {'sent_id', 'label', 'text'}\n"
          ]
        }
      ],
      "source": [
        "# Replace this URL with the raw URL of the file you want to fetch from GitHub\n",
        "train_url = \"https://raw.githubusercontent.com/ltgoslo/norec_sentence/main/binary/train.json\"\n",
        "test_url = \"https://raw.githubusercontent.com/ltgoslo/norec_sentence/main/binary/test.json\"\n",
        "\n",
        "# Fetch the content of the file\n",
        "train_response = requests.get(train_url)\n",
        "test_response = requests.get(test_url)\n",
        "\n",
        "# Raise an exception if there was an error fetching the file\n",
        "train_response.raise_for_status() \n",
        "test_response.raise_for_status()\n",
        "\n",
        "# Get the content of the file as a string\n",
        "json_training_data = json.loads(train_response.text)\n",
        "json_test_data = json.loads(test_response.text)\n",
        "\n",
        "# This is just for analytic reasons. Not for loading the content:\n",
        "data_analysys(json_training_data)\n",
        "data_analysys(json_test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.Loading the data & filtering out what's redundant"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j0EHx3IBpLK7"
      },
      "source": [
        "As you can see there's an extra, redundant label that I do not have to consider doing my sentiment analysis, and that is the \"sent_id\" label. The date the message was sent is irrelevant therefore I can remove it from my data to do more efficient model training. \n",
        "\n",
        "I will only be using the text and label part of this data so I continue in defining a function that reads only 'label' & 'text' part of the data. \n",
        "\n",
        "\n",
        "\"data_reading()\" extracts the data from the json file provided in a structured way: [\"sentance\", \"sentiment\"]. I consider the data not to complex so I decided to not bother with visualizing the data to much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TgPxQu2HpLK8"
      },
      "outputs": [],
      "source": [
        "def data_reading(json_data) -> list:\n",
        "    # Extract message_data directly using a list comprehension\n",
        "    message_data = [[category[\"text\"], category[\"label\"]] for category in json_data]\n",
        "\n",
        "    return message_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then I proceed to define the global variable to make sure that I only load this data once. This is done in order to make the processing much faster. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = data_reading(json_training_data)\n",
        "test_data = data_reading(json_test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.Handling & Praparing the data for my model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3_Y9pApLK9"
      },
      "source": [
        "Before creating the function I was struggling with accessing the data in an efficient way. I wanted to divide: the sanitizing, tokenizing, lemmatizing and vectorizing of the data. But I quickly figured that it would have cost a lot of computation time since I would have to access the same data multiple times in nested for loops. So instead I created preprocessing() which did sanitizing with tokenizing, and then lemmatized the sentances in one go. In addition to that, this function divides the preprocessed data into sentances and sentiments, which are perfectly prepared to be vectorized. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2PLrdkmFpLK9"
      },
      "outputs": [],
      "source": [
        "def preprocessing(data):\n",
        "    sentences = []\n",
        "    sentiments = []\n",
        "    lemmatizer = spacy.load(\"nb_core_news_sm\")  # Norwegian lemmatization model.\n",
        "\n",
        "    for chunk in data:\n",
        "        text, sentiment = chunk[0], chunk[1]\n",
        "        # Here I tokenize the data, and prepare it to run it through a sanitizer.\n",
        "        pre_lemmatized = lemmatizer(text)\n",
        "\n",
        "\n",
        "        # Here I add the lemmatized lowercase word to the \"lemmatized\" list,\n",
        "        # but only after it passes all the tests, to check if it's a legitimate word.  \n",
        "        lemmatized = [root_word.lemma_.lower() for root_word in pre_lemmatized\n",
        "                      if (not root_word.is_punct\n",
        "                          and not root_word.is_currency\n",
        "                          and not root_word.is_digit\n",
        "                          and not root_word.is_space\n",
        "                          and not root_word.is_stop\n",
        "                          and not root_word.like_num)]\n",
        "\n",
        "\n",
        "        # This if statement checks if the list is empty or not.\n",
        "        # If it is, it simply continues to the next sentance without adding the \n",
        "        # sentiment. That provides a reasurance that there will not be any sentiments\n",
        "        # attached to a empty list.\n",
        "        if len(lemmatized)==0:\n",
        "            continue\n",
        "        else:\n",
        "            # And here I join each processed word to one sentance in order to do\n",
        "            # sentance sentimental analysis.\n",
        "            sentence = ' '.join(lemmatized)\n",
        "            sentences.append(sentence)\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "    return sentences, sentiments\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "os-AnehYpLK-"
      },
      "source": [
        "In the last part of the for loop, where I have that if statement I figured that when filtering out all the stopwords all slang words may be filtered out too. So I needed to add that to not end up with some sentiments being attached to empty lists. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cZA3BJpwpLK_"
      },
      "source": [
        "So I performe the vectorization using CountVectorizer() & LabelEncoder() because that seemed most trivial of every other way that I stumbled upon. I considered if I should use TF-IDF or Bag-of-Words vectorization. But I concluded with TF-IDF seemed to be more fit for bigger datas with variaty of lenghts and inputs. Meanwhile here I have tree prepared documents with the same format which will not vary as much as data from the real world would. \n",
        "And my implementation is shown bellow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_iPKM_IapLK_"
      },
      "outputs": [],
      "source": [
        "def vectornihilation(x_axis, y_axis):\n",
        "    vectorizer = CountVectorizer()\n",
        "    labelizer = LabelEncoder()\n",
        "    X_matrix = vectorizer.fit_transform(x_axis)\n",
        "    Y_matrix = labelizer.fit_transform(y_axis)\n",
        "\n",
        "    return X_matrix, Y_matrix, vectorizer, labelizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luaJf8cspLK_"
      },
      "source": [
        "And this function gives me a matrix as the x vectors and a numpy array with vector representations of the labels as y."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "drzstF1hpLK_"
      },
      "source": [
        "### 4. Finding the optimal algorithm to performe sentance sentimental analysis.\n",
        "\n",
        "We considered specificly these tree models to implement because:\n",
        "Naive Bayes method was often implemented in sentimen analysis and could either have a very good score or very bad depending on your data type and preprocessing. This model also seemed as one of the closest to the types of models that the book implemented therefore I considered this model to be most relevant.\n",
        "\n",
        "Logistic Regression model is said to be a very precise model in general with very comprehensible outputs and was said to be easy to implement in general. The book also implements this method so I thought that I will have enough documentation on it to be able to understand it in depth.\n",
        "\n",
        "Decision Tree model is also a very popular option in sentimental analysis because of it's abilities of generalization. By that I mean that if you find a perfect grouping size for the model to use you can be able to predict sentiments in a very accurate way. \n",
        "\n",
        "Continuing:\n",
        "Now that I have preprocessed data I can use that tougheter with the GridSearchCV() method in order to find the best algorithm and the best parameters for that algorithm to predict the sentiments of the sentances. \n",
        "\n",
        "I do that by implementing exhoustive search on the parameters for each of the algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uxdYXglvpLLA"
      },
      "outputs": [],
      "source": [
        "def naiveBayes(x_ax, y_ax):\n",
        "    hypeOmeters = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "    \n",
        "    grid = GridSearchCV(GaussianNB(), hypeOmeters, cv=5)\n",
        "    grid.fit(x_ax, y_ax)\n",
        "    \n",
        "    return grid.best_params_['var_smoothing']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "My Naive Bayes function performes an exhoustive search looking for the optimal parameter var_smooth which addresses the problem called \"zero variance\" which basically addresses data where the variance is zero or almost zero (very small), and makes sure that the model can still make good prediction even when the data sets variance for each label is very small. \n",
        "\n",
        "Then the function returns the best parameter for Naive Bayes model that was estimated using our data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "n0eqRKwcpLLA"
      },
      "outputs": [],
      "source": [
        "def logisticRegression(x_ax, y_ax):\n",
        "    C_meters = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
        "    \n",
        "    grid = GridSearchCV(LogisticRegression(), C_meters, cv=5)\n",
        "    grid.fit(x_ax, y_ax)\n",
        "    \n",
        "    return grid.best_params_['C']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression uses the C parameter which decides how precise it should predict the the oucome given an input. If given a too small C parameter the Logistic Regression model would fit to perfectly to the test data whcih is also known as overfitting.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IRlgG8KapLLA"
      },
      "outputs": [],
      "source": [
        "def decissionTree(x_ax, y_ax):\n",
        "    grido_meters = { 'max_depth': [2, 4, 6, 8, 10], 'min_samples_split': [2, 5, 10, 20] }\n",
        "\n",
        "    grid = GridSearchCV(DecisionTreeClassifier(), grido_meters, cv=5)\n",
        "    grid.fit(x_ax, y_ax)\n",
        "    \n",
        "    max_depth, min_sample = grid.best_params_['max_depth'], grid.best_params_['min_samples_split']\n",
        "    return max_depth, min_sample\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decission tree tries to classify / generalize the input compared to label as much as possible. It creates consequances that each input vector has, which means it tries to understand the correlation between the \"label\" and the \"text\" by creating as pure groups of similarities between sentances with the same labels as possible. \n",
        "\n",
        "Therefore we must provide two of the hypermeters which are:\n",
        "\n",
        "max_depth which decides how finegrained the similarities between two sentances should be. (How deep in the details the models will investigate)\n",
        "\n",
        "min_sample_splits which decides how narrow the groups of different consequances should be. \n",
        "\n",
        "This function works much like the functions defined above by exhoustive search for the optimal hypermeter and then returning the one witht he best score. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_dXCuBpLLA"
      },
      "source": [
        "Here we use a general function that goes through each function automaticly registrating their best parameter. We will use this function to find out which of the models is most precise with least divation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  optimal_paramteres() returns the optimal hyperparameters for each model\n",
        "def optimal_paramteres(sentances, sentiments, y_axis, densed_X):\n",
        "    Logistic_param = logisticRegression(\n",
        "    x_ax = densed_X, \n",
        "    y_ax = y_axis\n",
        "    )\n",
        "\n",
        "    Naive_param = naiveBayes(\n",
        "        x_ax = densed_X, \n",
        "        y_ax = y_axis\n",
        "    )\n",
        "\n",
        "    DecTree_depth_param, DecTree_sample_param = decissionTree(\n",
        "        x_ax = densed_X, \n",
        "        y_ax = y_axis\n",
        "    )\n",
        "\n",
        "    return [Logistic_param, Naive_param, DecTree_depth_param, DecTree_sample_param]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m x_ax, y_ax, vectorizer, labelizer \u001b[39m=\u001b[39m vectornihilation(sentances, sentiments)\n\u001b[0;32m      4\u001b[0m densed_X \u001b[39m=\u001b[39m x_ax\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m----> 6\u001b[0m optimal_paramteres \u001b[39m=\u001b[39m optimal_paramteres(\n\u001b[0;32m      7\u001b[0m     sentances \u001b[39m=\u001b[39;49m sentances,\n\u001b[0;32m      8\u001b[0m     sentiments \u001b[39m=\u001b[39;49m sentiments,\n\u001b[0;32m      9\u001b[0m     y_axis \u001b[39m=\u001b[39;49m y_ax,\n\u001b[0;32m     10\u001b[0m     densed_X \u001b[39m=\u001b[39;49m densed_X\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[39m# Extracting the parameters from the list returned by optimal_paramteres()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logistic_param, naive_param, decTree_depth_param, decTree_sample_param \u001b[39m=\u001b[39m optimal_paramteres\n",
            "Cell \u001b[1;32mIn[28], line 13\u001b[0m, in \u001b[0;36moptimal_paramteres\u001b[1;34m(sentances, sentiments, y_axis, densed_X)\u001b[0m\n\u001b[0;32m      3\u001b[0m Logistic_param \u001b[39m=\u001b[39m logisticRegression(\n\u001b[0;32m      4\u001b[0m x_ax \u001b[39m=\u001b[39m densed_X, \n\u001b[0;32m      5\u001b[0m y_ax \u001b[39m=\u001b[39m y_axis\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m Naive_param \u001b[39m=\u001b[39m naiveBayes(\n\u001b[0;32m      9\u001b[0m     x_ax \u001b[39m=\u001b[39m densed_X, \n\u001b[0;32m     10\u001b[0m     y_ax \u001b[39m=\u001b[39m y_axis\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m DecTree_depth_param, DecTree_sample_param \u001b[39m=\u001b[39m decissionTree(\n\u001b[0;32m     14\u001b[0m     x_ax \u001b[39m=\u001b[39;49m densed_X, \n\u001b[0;32m     15\u001b[0m     y_ax \u001b[39m=\u001b[39;49m y_axis\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m [Logistic_param, Naive_param, DecTree_depth_param, DecTree_sample_param]\n",
            "Cell \u001b[1;32mIn[27], line 5\u001b[0m, in \u001b[0;36mdecissionTree\u001b[1;34m(x_ax, y_ax)\u001b[0m\n\u001b[0;32m      2\u001b[0m grido_meters \u001b[39m=\u001b[39m { \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m10\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmin_samples_split\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m] }\n\u001b[0;32m      4\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(DecisionTreeClassifier(), grido_meters, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m grid\u001b[39m.\u001b[39;49mfit(x_ax, y_ax)\n\u001b[0;32m      7\u001b[0m max_depth, min_sample \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbest_params_[\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m], grid\u001b[39m.\u001b[39mbest_params_[\u001b[39m'\u001b[39m\u001b[39mmin_samples_split\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m max_depth, min_sample\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Here we use the best parameters obtain from each of the functions defined.\n",
        "sentances , sentiments = preprocessing(train_data)\n",
        "x_ax, y_ax, vectorizer, labelizer = vectornihilation(sentances, sentiments)\n",
        "densed_X = x_ax.toarray()\n",
        "\n",
        "optimal_paramteres = optimal_paramteres(\n",
        "    sentances = sentances,\n",
        "    sentiments = sentiments,\n",
        "    y_axis = y_ax,\n",
        "    densed_X = densed_X\n",
        ")\n",
        "\n",
        "# Extracting the parameters from the list returned by optimal_paramteres()\n",
        "logistic_param, naive_param, decTree_depth_param, decTree_sample_param = optimal_paramteres"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the scores parameters obtained we obtained the best possible score from each of choosen functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYs6uLNGpLLB"
      },
      "outputs": [],
      "source": [
        "# This function will return the best cross validation score \n",
        "# which we will use to evaluate which function performes best.\n",
        "def OptimalSentimentAnalysis(densed_X, y_axis, model, param1, param2=None):\n",
        "    if param2 is None:\n",
        "        if model == GaussianNB:\n",
        "            Cross_Validation = cross_val_score(\n",
        "                model(var_smoothing=param1),\n",
        "                densed_X,\n",
        "                y_axis\n",
        "            )\n",
        "        elif model == LogisticRegression:\n",
        "            Cross_Validation = cross_val_score(\n",
        "                model(C=param1),\n",
        "                densed_X,\n",
        "                y_axis\n",
        "            )\n",
        "    else:\n",
        "        if model == DecisionTreeClassifier:\n",
        "            Cross_Validation = cross_val_score(\n",
        "                model(max_depth=param1, min_samples_split=param2),\n",
        "                densed_X,\n",
        "                y_axis\n",
        "            )\n",
        "    return Cross_Validation\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def printCrossInfo(model):\n",
        "    print(f'Model: {type(model).__name__} | Cross-validation score: {model.mean()}')\n",
        "    print(f'Model: {type(model).__name__} | Cross-validation standard deviation: {model.std()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
=======
      "execution_count": 33,
>>>>>>> b826e2159bd989f464a5fc09e355f73d6803ac14
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the cross validation score for each model\n",
        "\n",
        "nb_score = OptimalSentimentAnalysis(\n",
        "    densed_X = densed_X,\n",
        "    y_axis = y_ax,\n",
        "    model = GaussianNB,\n",
        "    param1 = naive_param\n",
        ")\n",
        "\n",
        "lr_score = OptimalSentimentAnalysis(\n",
        "    densed_X = densed_X,\n",
        "    y_axis = y_ax,\n",
        "    model = LogisticRegression,\n",
        "    param1 = logistic_param\n",
        ")\n",
        "\n",
        "dtc_score = OptimalSentimentAnalysis(\n",
        "    densed_X = densed_X,\n",
        "    y_axis = y_ax,\n",
        "    model = DecisionTreeClassifier,\n",
        "    param1 = decTree_depth_param,\n",
        "    param2 = decTree_sample_param\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: GaussianNB\n",
            "Cross-validation mean: 0.5829\n",
            "Cross-validation standard deviation: 0.0101\n",
            "\n",
            "Model: LogisticRegression\n",
            "Cross-validation mean: 0.7079\n",
            "Cross-validation standard deviation: 0.0082\n",
            "\n",
            "Model: DecisionTreeClassifier\n",
            "Cross-validation mean: 0.6922\n",
            "Cross-validation standard deviation: 0.0064\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Getting the cross validation score for each model\n",
        "models_info = [\n",
        "    (\"GaussianNB\", nb_score),\n",
        "    (\"LogisticRegression\", lr_score),\n",
        "    (\"DecisionTreeClassifier\", dtc_score)\n",
        "]\n",
        "\n",
        "for model_name, scores in models_info:\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Cross-validation mean: {scores.mean():.4f}\")\n",
        "    print(f\"Cross-validation standard deviation: {scores.std():.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_best_model(model_scores_list):\n",
        "    best_model = None\n",
        "    best_mean_score = -1\n",
        "    best_params = None\n",
        "\n",
        "    for model, scores, params in model_scores_list:\n",
        "        mean_score = scores.mean()\n",
        "        if mean_score > best_mean_score:\n",
        "            best_model = model\n",
        "            best_mean_score = mean_score\n",
        "            best_params = params\n",
        "\n",
        "    return best_model, best_mean_score, best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
>>>>>>> b826e2159bd989f464a5fc09e355f73d6803ac14
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model: LogisticRegression with mean score: 0.7079115397406053\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "model_scores_list = [\n",
        "    (GaussianNB, nb_score, naive_param),\n",
        "    (LogisticRegression, lr_score, logistic_param),\n",
        "    (DecisionTreeClassifier, dtc_score, (decTree_depth_param, decTree_sample_param))\n",
        "]\n",
        "\n",
        "best_model, best_mean_score, best_params = select_best_model(model_scores_list)\n",
        "print(f'Best model: {best_model.__name__} with mean score: {best_mean_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mp06kekpLLB"
      },
      "source": [
        "Here I quickly explain what different parameters does in the different models:\n",
        "var_smoothing variable chooses how smoothly the variance should be distributed in the Naive Bayes distribution model.\n",
        "\n",
        "C is a value that regularates the complexity of the Logistic Regregression model. What I try to find is the maximum Likelyhood parameter. Which is the parameter that gives me the optimal values for the likelyhood of getting Y given an X.\n",
        "\n",
        "max_depth is the maximum depth of the decision tree. A decision tree can become very complex and can overfit the training data if it is allowed to grow too deep. So, setting a maximum depth for the tree can help to prevent overfitting and improve generalization to new data.\n",
        "\n",
        "min_samples_split chooses how small the decision groups can be, preventing this parameter from being to small can help in preventing overfitting by creating a more general model that fits more data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILymt7WPpLLB"
      },
      "source": [
        "And now I that I have found that Logistic Regression is the model that I am supposed to use I can validate the accuracy of the model before testing it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def best_model_train_and_test(train_x, train_y, test_x, test_y, model_class, params):\n",
        "    if model_class == DecisionTreeClassifier:\n",
        "        max_depth, min_samples_split = params\n",
        "        model = model_class(max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    elif model_class == GaussianNB:\n",
        "        var_smoothing = params\n",
        "        model = model_class(var_smoothing=var_smoothing)\n",
        "    elif model_class == LogisticRegression:\n",
        "        C_param = params\n",
        "        model = model_class(C=C_param)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_x, train_y)\n",
        "\n",
        "    # Test the model\n",
        "    test_predictions = model.predict(test_x)\n",
        "\n",
        "    accuracy = accuracy_score(test_y, test_predictions)\n",
        "    report = classification_report(test_y, test_predictions)\n",
        "\n",
        "    return (f\"{model_class.__name__}:\\nAccuracy: {accuracy:.4f}\\n{report}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_best_model():\n",
        "    train_sentences, train_sentiments = preprocessing(data_reading(json_training_data))\n",
        "    test_sentences, test_sentiments = preprocessing(data_reading(json_test_data))\n",
        "\n",
        "    x_train, y_train, vectorizer, labelizer = vectornihilation(train_sentences, train_sentiments)\n",
        "    x_test = vectorizer.transform(test_sentences)\n",
        "    y_test = labelizer.transform(test_sentiments)\n",
        "\n",
        "    dense_x_train = x_train.toarray()\n",
        "    dense_x_test = x_test.toarray()\n",
        "\n",
        "    return best_model_train_and_test(\n",
        "        dense_x_train,\n",
        "        y_train,\n",
        "        dense_x_test,\n",
        "        y_test,\n",
        "        best_model,\n",
        "        best_params\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression:\n",
            "Accuracy: 0.7153\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.28      0.38       182\n",
            "           1       0.74      0.91      0.82       401\n",
            "\n",
            "    accuracy                           0.72       583\n",
            "   macro avg       0.66      0.60      0.60       583\n",
            "weighted avg       0.69      0.72      0.68       583\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(evaluate_best_model())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d5a8d3d6fa4a8c42668baa170a0b6198815339ab86606f8fa92c48503601c5fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
