{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "category_index = 8\n",
    "n_val = 5000\n",
    "\n",
    "data_path = '/cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function for loading the Cifar10 dataset.\n",
    "\n",
    "The method will have to be run twice.\n",
    "After running the method for the first time we get create a normalizer from the std and mean of the images. The method is then ran for a second time with the normalizer as the preprocessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading the CIFAR-10 dataset as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformed_cifar10_train_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stacking the set of images into a single tensor. We then create a normalizer for the dataset around the mean and standard deviation of the 3 dimensions (height, width channel (color))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in transformed_cifar10_train_val])\n",
    "\n",
    "normalizer = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = imgs.mean(dim=(0, 2, 3)), std = imgs.std(dim=(0, 2, 3)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading the dataset as tensors for training+validation and testing. This time we apply the composition of transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_cifar10_train_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform = normalizer\n",
    ")\n",
    "\n",
    "\n",
    "transformed_cifar10_test = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform = normalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As this is a binary classification problem where we only want to identify whether an image is a ship or not, we can set the labels that are \"ship\" to true. We set all other labels to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = np.array([label for _, label in transformed_cifar10_train_val])\n",
    "train_labels = np.array(train_labels == category_index).astype(int)\n",
    "\n",
    "test_labels = np.array([label for _, label in transformed_cifar10_test])\n",
    "test_labels = np.array(test_labels == category_index).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Splitting the training and validation set randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train dataset:         45000\n",
      "Size of the validation dataset:    5000\n",
      "Size of the test dataset:          10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({8: 4486,\n",
       "         7: 4486,\n",
       "         1: 4512,\n",
       "         6: 4510,\n",
       "         3: 4516,\n",
       "         5: 4487,\n",
       "         4: 4499,\n",
       "         2: 4483,\n",
       "         0: 4518,\n",
       "         9: 4503})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = len(transformed_cifar10_train_val)-n_val\n",
    "\n",
    "transformed_cifar10_train_split, transformed_cifar10_val_split = random_split(\n",
    "    transformed_cifar10_train_val,\n",
    "    [n_train, n_val],\n",
    "\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "print(\"Size of the train dataset:        \", len(transformed_cifar10_train_split))\n",
    "print(\"Size of the validation dataset:   \", len(transformed_cifar10_val_split))\n",
    "print(\"Size of the test dataset:         \", len(transformed_cifar10_test))\n",
    "\n",
    "Counter([label for _, label in transformed_cifar10_train_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Choosing a pre-trained CNN model: we chose ResNet18, which is not trained on Cifar-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading pre-trained ResNet18 model and modifying the last layer. We are doing binary classification, so we think we only need one node in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use Binary Cross Entropy as it should be suitable for binary classification problems (add reasoning and explanation). We use nn.BCEWithLogitsLoss() as it combines the sigmoid activation function and the BCE into a single class.\n",
    "\n",
    "The optimizer we use is Adam, and we will begin with a learning rate of 0.001, just because it is a commonly used learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),  lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 548.36 seconds, Loss: -421282.4158893035, Validation Loss: -509482.16182324843\n",
      "Epoch 2, Time: 570.23 seconds, Loss: -647558.0241871002, Validation Loss: -733722.125995223\n",
      "Epoch 3, Time: 561.61 seconds, Loss: -913919.2992626155, Validation Loss: -1037977.1313694267\n",
      "Epoch 4, Time: 576.80 seconds, Loss: -1218624.665156361, Validation Loss: -1257535.8618630574\n",
      "Epoch 5, Time: 578.42 seconds, Loss: -1568220.926350391, Validation Loss: -1552223.9136146498\n",
      "Epoch 6, Time: 560.75 seconds, Loss: -1961719.10083511, Validation Loss: -2150417.268710191\n",
      "Epoch 7, Time: 579.78 seconds, Loss: -2395640.4633972994, Validation Loss: -2372017.185509554\n",
      "Epoch 8, Time: 548.80 seconds, Loss: -2875821.774697939, Validation Loss: -2868523.8829617836\n",
      "Epoch 9, Time: 562.29 seconds, Loss: -3394832.891169154, Validation Loss: -3367133.5031847134\n",
      "Epoch 10, Time: 559.09 seconds, Loss: -3956105.931414357, Validation Loss: -4125616.933121019\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "\n",
    "train_loader = DataLoader(transformed_cifar10_train_split, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(transformed_cifar10_val_split, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = loss_function(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_function(outputs, labels.float())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Epoch {epoch + 1}, Time: {epoch_time:.2f} seconds, Loss: {running_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
